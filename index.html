<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"unicorn-raya.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="摸鱼小据点">
<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://unicorn-raya.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="摸鱼小据点">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="ray">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://unicorn-raya.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Hexo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">ray</p>
  <div class="site-description" itemprop="description">摸鱼小据点</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://unicorn-raya.github.io/2024/09/30/skills/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ray">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="摸鱼小据点">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/09/30/skills/" class="post-title-link" itemprop="url">如何用python效率往mysql写入1000w条假数据</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-09-30 10:47:13" itemprop="dateCreated datePublished" datetime="2024-09-30T10:47:13+08:00">2024-09-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-10-02 22:46:37" itemprop="dateModified" datetime="2024-10-02T22:46:37+08:00">2024-10-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <span id="more"></span>
<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>一个压力测试的情况，需要一次性查询查询大量数据，然而数据库中没有这么多数据。因此为了测试场景要求：</p>
<ol>
<li>数据的行行之间不可以重复</li>
<li>快速写入（时间不够了）</li>
</ol>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>我们用逐条写入MySQL的模式，效率非常底下，效率表大概如下:</p>
<table>
<thead>
<tr>
<th>数据量</th>
<th>时间</th>
</tr>
</thead>
<tbody><tr>
<td>10w数据</td>
<td>2min24.6s</td>
</tr>
<tr>
<td>100w数据</td>
<td>22min31.7s</td>
</tr>
<tr>
<td>1000w数据</td>
<td>220min 7.2s</td>
</tr>
<tr>
<td>建表语句：</td>
<td></td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS morkusers (</span><br><span class="line">    id INT PRIMARY KEY,</span><br><span class="line">    name VARCHAR(255),</span><br><span class="line">    gender VARCHAR(255),</span><br><span class="line">    age INT,</span><br><span class="line">    date VARCHAR(255)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>此时的完整代码为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">from faker import Faker</span><br><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line"># MySQL 数据库连接配置</span><br><span class="line">connection = pymysql.connect(</span><br><span class="line">    host=&#x27;localhost&#x27;,</span><br><span class="line">    user=&#x27;root&#x27;,</span><br><span class="line">    password=&#x27;123456&#x27;,</span><br><span class="line">    database=&#x27;morkdata&#x27;</span><br><span class="line">)</span><br><span class="line">try:</span><br><span class="line">    with connection.cursor() as cursor:</span><br><span class="line">        # 创建表（如果表不存在）</span><br><span class="line">        create_table_query = &quot;&quot;&quot;</span><br><span class="line">        CREATE TABLE IF NOT EXISTS morkusers (</span><br><span class="line">            id INT PRIMARY KEY,</span><br><span class="line">            name VARCHAR(255),</span><br><span class="line">            gender VARCHAR(255),</span><br><span class="line">            age INT,</span><br><span class="line">            date VARCHAR(255)</span><br><span class="line">        );</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        cursor.execute(create_table_query)</span><br><span class="line">        # 创建 Faker 对象</span><br><span class="line">        faker = Faker()</span><br><span class="line">        N = 10000000</span><br><span class="line">        for _ in range(N):</span><br><span class="line">            id_ = _ + 1</span><br><span class="line">            name = faker.name()</span><br><span class="line">            gender = faker.random_element(elements=(&#x27;Male&#x27;, &#x27;Female&#x27;,&#x27;unknown&#x27;))</span><br><span class="line">            age = faker.random_int(min=1, max=60)</span><br><span class="line">            random_date = faker.date_between(start_date=&#x27;-10y&#x27;, end_date=&#x27;today&#x27;)</span><br><span class="line">            insert_query = &quot;&quot;&quot;</span><br><span class="line">            INSERT INTO morkusers (id, name, gender, age, date)</span><br><span class="line">            VALUES (%s, %s, %s, %s, %s);</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">            cursor.execute(insert_query,(id_, name, gender, age, random_date))</span><br><span class="line"></span><br><span class="line">            # 提交事务</span><br><span class="line">        connection.commit()</span><br><span class="line">finally:</span><br><span class="line">    # 关闭连接</span><br><span class="line">    connection.close()</span><br></pre></td></tr></table></figure>
<p>此时我们可以对数据的插入过程进行优化，主要两个部分:   </p>
<ol>
<li>单条提交 改为 多条提交</li>
<li>db的引擎：默认的innodb 改为 MyIsAm</li>
</ol>
<p>关于第二条，我们通过修改建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS morkusers (</span><br><span class="line">    id INT PRIMARY KEY,</span><br><span class="line">    name VARCHAR(255),</span><br><span class="line">    gender VARCHAR(255),</span><br><span class="line">    age INT,</span><br><span class="line">    date VARCHAR(255)</span><br><span class="line">) ENGINE=MyISAM DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure>
<p>关于第一条,我们将代码改成如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">from faker import Faker</span><br><span class="line">import pymysql</span><br><span class="line"># MySQL 数据库连接配置</span><br><span class="line">connection = pymysql.connect(</span><br><span class="line">    host=&#x27;localhost&#x27;,</span><br><span class="line">    user=&#x27;root&#x27;,</span><br><span class="line">    password=&#x27;123456&#x27;,</span><br><span class="line">    database=&#x27;morkdata&#x27;</span><br><span class="line">)</span><br><span class="line">try:</span><br><span class="line">    with connection.cursor() as cursor:</span><br><span class="line">        # 创建表（如果表不存在）</span><br><span class="line">        create_table_query = &quot;&quot;&quot;</span><br><span class="line">        CREATE TABLE IF NOT EXISTS morkusers (</span><br><span class="line">            id INT PRIMARY KEY,</span><br><span class="line">            name VARCHAR(255),</span><br><span class="line">            gender VARCHAR(255),</span><br><span class="line">            age INT,</span><br><span class="line">            date VARCHAR(255)</span><br><span class="line">        ) ENGINE=MyISAM DEFAULT CHARSET=utf8;</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        cursor.execute(create_table_query)</span><br><span class="line">        # 创建 Faker 对象</span><br><span class="line">        faker = Faker()</span><br><span class="line">        step = 1000</span><br><span class="line">        N = step * 10000</span><br><span class="line"></span><br><span class="line">        for s in range(0,N,step):</span><br><span class="line">            tmp_data = [] </span><br><span class="line">            for t in range(step):</span><br><span class="line">                id_ = s + t + 1</span><br><span class="line">                name = faker.name()</span><br><span class="line">                gender = faker.random_element(elements=(&#x27;Male&#x27;, &#x27;Female&#x27;,&#x27;unknown&#x27;))</span><br><span class="line">                age = faker.random_int(min=1, max=60)</span><br><span class="line">                date = faker.date_between(start_date=&#x27;-10y&#x27;, end_date=&#x27;today&#x27;)</span><br><span class="line">                tmp_data.append((id_,name,gender,age,date))</span><br><span class="line">            </span><br><span class="line">            insert_query = &quot;&quot;&quot;</span><br><span class="line">            INSERT INTO morkusers (id, name, gender, age, date)</span><br><span class="line">            VALUES (%s, %s, %s, %s, %s);</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">            cursor.executemany(insert_query,tmp_data)</span><br><span class="line">            # 提交事务</span><br><span class="line">            connection.commit()</span><br><span class="line">finally:</span><br><span class="line">    # 关闭连接</span><br><span class="line">    connection.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>此时的时间统计为：</p>
<table>
<thead>
<tr>
<th>数据量</th>
<th>时间</th>
</tr>
</thead>
<tbody><tr>
<td>10w数据</td>
<td>27.8s</td>
</tr>
<tr>
<td>100w数据</td>
<td>4min57.5</td>
</tr>
<tr>
<td>1000w数据</td>
<td>42min40.4s</td>
</tr>
</tbody></table>
<p>再通过对N和step调整，让Step &#x3D; 10000，即每次写10000条，可以得到以下效率</p>
<table>
<thead>
<tr>
<th>数据量</th>
<th>时间</th>
<th>step</th>
</tr>
</thead>
<tbody><tr>
<td>1000w数据</td>
<td>38min28.4s</td>
<td>10000</td>
</tr>
<tr>
<td>1000w数据</td>
<td>40min13.2s</td>
<td>10000000</td>
</tr>
</tbody></table>
<p>其实可以发现，批量写入的均值也就是40min上下波动了</p>
<h2 id="MySqlBulkLoader-LOAD-DATA-INFILE"><a href="#MySqlBulkLoader-LOAD-DATA-INFILE" class="headerlink" title="MySqlBulkLoader&#x2F;LOAD DATA INFILE"></a>MySqlBulkLoader&#x2F;LOAD DATA INFILE</h2><p>TL;DR,三篇文章   </p>
<ol>
<li><a target="_blank" rel="noopener" href="https://mariadb.com/kb/en/how-to-quickly-insert-data-into-mariadb/">How to Quickly Insert Data Into MariaDB</a>   </li>
<li><a target="_blank" rel="noopener" href="https://mariadb.com/kb/en/load-data-infile/">LOAD DATA INFILE</a>   </li>
<li><a target="_blank" rel="noopener" href="https://mariadb.com/kb/en/mysqlimport/">mysqlimport</a></li>
</ol>
<p>和 <a target="_blank" rel="noopener" href="https://blog.restkhz.com/post/quickly-insert-data-into-mysql">一个可靠大哥的文章</a></p>
<p>一个最直接的法子就用以下法子来写入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS morkusers (</span><br><span class="line">    id INT PRIMARY KEY,</span><br><span class="line">    name VARCHAR(255),</span><br><span class="line">    gender VARCHAR(255),</span><br><span class="line">    age INT,</span><br><span class="line">    date VARCHAR(255)</span><br><span class="line">);</span><br><span class="line">LOAD DATA LOCAL INFILE &#x27;/root/data/fake_data.csv&#x27; INTO TABLE morkusers FIELDS TERMINATED BY &#x27;,&#x27; ;</span><br><span class="line">;</span><br></pre></td></tr></table></figure>
<p>发现报错 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(3948, &#x27;Loading local data is disabled; this must be enabled on both the client and server sides&#x27;)</span><br></pre></td></tr></table></figure>
<p>由于电脑的mysql是docker服务，因此修改了配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">show global variables like &#x27;local_infile&#x27; -- 默认是0</span><br><span class="line">set global local_infile = 1;</span><br></pre></td></tr></table></figure>

<p>最终代码为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from faker import Faker</span><br><span class="line">import pandas as pd</span><br><span class="line">faker = Faker()</span><br><span class="line">tmp_data = []</span><br><span class="line">step = 10000000</span><br><span class="line">N = step * 1</span><br><span class="line">data = &#123;</span><br><span class="line">    &#x27;name&#x27;: [faker.name() for _ in range(N)],</span><br><span class="line">    &#x27;gender&#x27;: [ faker.random_element(elements=(&#x27;Male&#x27;, &#x27;Female&#x27;,&#x27;unknown&#x27;)) for _ in range(N)],</span><br><span class="line">    &#x27;age&#x27;: [faker.random_int(min=1, max=60) for _ in range(N)],</span><br><span class="line">    &#x27;date&#x27;: [ faker.date_between(start_date=&#x27;-10y&#x27;, end_date=&#x27;today&#x27;) for _ in range(N)]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">df.to_csv(&#x27;/mnt/c/mysql_data/fake_data.csv&#x27;, index = False)</span><br></pre></td></tr></table></figure>

<p>以及</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS morkusers (</span><br><span class="line">    id INT PRIMARY KEY,</span><br><span class="line">    name VARCHAR(255),</span><br><span class="line">    gender VARCHAR(255),</span><br><span class="line">    age INT,</span><br><span class="line">    date VARCHAR(255)</span><br><span class="line">); </span><br><span class="line"></span><br><span class="line">LOAD DATA LOCAL INFILE &#x27;/mysql_data/morkdata.csv&#x27; </span><br><span class="line">INTO TABLE morkusers FIELDS TERMINATED BY &#x27;,&#x27;</span><br><span class="line">LINES TERMINATED BY &#x27;\n&#x27;</span><br><span class="line">IGNORE 1 LINES</span><br><span class="line">(name,gender,age,date);</span><br></pre></td></tr></table></figure>

<p>效率表:</p>
<table>
<thead>
<tr>
<th>数据量</th>
<th>时间</th>
<th>步骤</th>
</tr>
</thead>
<tbody><tr>
<td>1000w数据</td>
<td>35minn52.8s</td>
<td>1000w假数据写本地</td>
</tr>
<tr>
<td>1000w数据</td>
<td>21:29.24 - 21:28:22 &#x3D; 58s</td>
<td>load到mysql中</td>
</tr>
</tbody></table>
<p>&#x3D; &#x3D;其实发现慢的是假数据生成的逻辑，导入数据的速度其实很快，<br>假数据的生成逻辑后续再考虑优化</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://unicorn-raya.github.io/2024/08/29/interview2024/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ray">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="摸鱼小据点">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/08/29/interview2024/" class="post-title-link" itemprop="url">面试记录</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-08-29 17:47:13" itemprop="dateCreated datePublished" datetime="2024-08-29T17:47:13+08:00">2024-08-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-10-01 19:42:04" itemprop="dateModified" datetime="2024-10-01T19:42:04+08:00">2024-10-01</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><h3 id="面试公司：gate-io"><a href="#面试公司：gate-io" class="headerlink" title="面试公司：gate.io"></a>面试公司：gate.io</h3><h4 id="1面"><a href="#1面" class="headerlink" title="1面"></a>1面</h4><p>Q1: 自我介绍一下<br>Q2: 数据测试的注意点有哪些<br>方向不对，pass了</p>
<h3 id="公司：金证，外派港交易所"><a href="#公司：金证，外派港交易所" class="headerlink" title="公司：金证，外派港交易所"></a>公司：金证，外派港交易所</h3><h4 id="简历通过，他们做web手工测试的，方向不对"><a href="#简历通过，他们做web手工测试的，方向不对" class="headerlink" title="简历通过，他们做web手工测试的，方向不对"></a>简历通过，他们做web手工测试的，方向不对</h4><h3 id="公司：-畅销家（深圳）网络科技有限公司"><a href="#公司：-畅销家（深圳）网络科技有限公司" class="headerlink" title="公司： 畅销家（深圳）网络科技有限公司"></a>公司： 畅销家（深圳）网络科技有限公司</h3><h4 id="简历通过，还没call"><a href="#简历通过，还没call" class="headerlink" title="简历通过，还没call"></a>简历通过，还没call</h4><h3 id="公司：-福里斯信息技术"><a href="#公司：-福里斯信息技术" class="headerlink" title="公司： 福里斯信息技术"></a>公司： 福里斯信息技术</h3><h4 id="简历通过，还没call-1"><a href="#简历通过，还没call-1" class="headerlink" title="简历通过，还没call"></a>简历通过，还没call</h4><h3 id="公司：软通动力"><a href="#公司：软通动力" class="headerlink" title="公司：软通动力"></a>公司：软通动力</h3><p>jd：</p>
<ul>
<li>1.具备大数据测试经验：数据仓库、hadoop、对象存储、文件系统、关系型数据库、NoSQL、消息系统、搜索等数据源测试</li>
<li>2.有丰富的接口测试经验，熟悉接口的请求方式，结构，响应</li>
<li>3.有webui自动化测试用例编写经验，如java+playright，python+selenium，js+puppeteer等</li>
<li>4.熟练使用jmeter</li>
<li>5.熟悉常用的linux命令</li>
<li>6.有大数据测试经验的优先</li>
</ul>
<h4 id="1面-1"><a href="#1面-1" class="headerlink" title="1面"></a>1面</h4><p>自我介绍<br>数据测试是怎么做的，准确性怎么保障<br>hadoop全家桶了解么<br>sql优化做过么，大致的流程<br>SQL优化的大致流程：</p>
<ol>
<li><p>分析SQL执行计划：使用EXPLAIN命令查看SQL语句的执行计划，了解索引使用情况、表连接方式等。</p>
</li>
<li><p>识别性能瓶颈：根据执行计划，找出耗时较长的操作，如全表扫描、低效的索引使用等。</p>
</li>
<li><p>优化索引：</p>
<ul>
<li>为常用查询条件创建合适的索引</li>
<li>避免过多索引，可能影响写入性能</li>
<li>考虑使用复合索引优化多条件查询</li>
</ul>
</li>
<li><p>优化查询语句：</p>
<ul>
<li>避免使用SELECT *，只查询需要的字段</li>
<li>使用LIMIT限制结果集大小</li>
<li>优化JOIN操作，考虑表的连接顺序</li>
<li>使用EXISTS代替IN，特别是大数据量时</li>
</ul>
</li>
<li><p>优化表结构：</p>
<ul>
<li>选择合适的数据类型，如使用INT而非VARCHAR存储数字</li>
<li>适当进行表分区或分表</li>
</ul>
</li>
<li><p>使用缓存：</p>
<ul>
<li>利用数据库查询缓存</li>
<li>考虑使用应用层缓存，如Redis</li>
</ul>
</li>
<li><p>优化数据库配置：</p>
<ul>
<li>调整数据库参数，如缓冲池大小、连接数等</li>
</ul>
</li>
<li><p>重写SQL：</p>
<ul>
<li>考虑使用子查询、临时表等方式重写复杂SQL</li>
</ul>
</li>
<li><p>定期维护：</p>
<ul>
<li>更新统计信息</li>
<li>定期进行碎片整理</li>
</ul>
</li>
<li><p>监控和持续优化：</p>
<ul>
<li>使用性能监控工具持续跟踪SQL执行情况</li>
<li>根据业务变化和数据增长情况，定期回顾和调整优化策略</li>
</ul>
</li>
</ol>
<p>数据质量是怎么保障<br>数据测试流程是啥样的<br>接口测试有做过么，接口测试的流程是什么<br>接口测试是怎么执行的<br>压力测试和性能测试有做过么<br>ui自动化测试有做过么<br>开发认为bug不算bug，应该怎么沟通<br>云服务有用过哪些<br>有没有做过数据的安全的测试，数据的安全性如何保障   </p>
<h3 id="公司：跨越速递"><a href="#公司：跨越速递" class="headerlink" title="公司：跨越速递"></a>公司：跨越速递</h3><h4 id="1面-2"><a href="#1面-2" class="headerlink" title="1面"></a>1面</h4><p>笔试四道题，最后一题会有分组的取优先数据的</p>
<p>1000w数据，用Python怎么写到MySQL</p>
<p>Hdfs上传数据<br>报表自动化测试<br>接口返回数据意思100 200 300 400<br>测试流程<br>如果产品对业务需求理解有问题，开发开发出了错误的代码，怎么修正<br>写水杯用例<br>笔试题思路<br>你作为测试比别的测试有啥优势<br>怎么判断sql性能，其实就是判断代码性能<br>数据测试的sql逻辑，如何确保测试覆盖率？（业务逻辑用的流程图，用流程图来计算覆盖率）<br>如何判断这个需求有可执行性<br>如何判断这个需求可以执行<br>工作最晚的一次是哪个项目，原因<br>Hdfs全家桶用过么，<br>Hive的分组函数哪些，如果分组函数取到了随机数，怎么指定唯一数<br>MapReduce的原理<br>你们报表怎么测试的<br>数仓有那几层<br>你们怎么测etl逻辑的<br>要求每一行都不重复<br>你们造数怎么造的<br>怎么保证数据测试逻辑全覆盖<br>流数据测试的重点<br>上下游依赖怎么判断</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="自动化测试中，如何解决Case依赖？"><a href="#自动化测试中，如何解决Case依赖？" class="headerlink" title="自动化测试中，如何解决Case依赖？"></a>自动化测试中，如何解决Case依赖？</h3><p><img src="/2024/08/29/interview2024/p1.png" alt="p1"><br><img src="/2024/08/29/interview2024/p2.png" alt="p2">    </p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkwNjMwMTgzMQ==&mid=2247490262&idx=1&sn=a67f610afa984ecca130a54a3be453ab&scene=21#wechat_redirect">MYSQL SQL 优化</a></p>
<p><a target="_blank" rel="noopener" href="https://brightliao.com/2021/04/20/data-testing/">数据测试实践</a></p>
<p><a target="_blank" rel="noopener" href="https://www.yuque.com/liguohui-3b9d9/ggi8sz/zgkmwi">测开面试</a></p>
<h3 id="你们公司业务中，自动化和手工分别占比多少？分别用来做什么业务？"><a href="#你们公司业务中，自动化和手工分别占比多少？分别用来做什么业务？" class="headerlink" title="你们公司业务中，自动化和手工分别占比多少？分别用来做什么业务？"></a>你们公司业务中，自动化和手工分别占比多少？分别用来做什么业务？</h3><h3 id="接口自动化中加解密如何处理？"><a href="#接口自动化中加解密如何处理？" class="headerlink" title="接口自动化中加解密如何处理？"></a>接口自动化中加解密如何处理？</h3><h3 id="cicd的概念，怎么执行的，执行的过程是啥样"><a href="#cicd的概念，怎么执行的，执行的过程是啥样" class="headerlink" title="cicd的概念，怎么执行的，执行的过程是啥样"></a>cicd的概念，怎么执行的，执行的过程是啥样</h3><h3 id="CI-CD的概念、执行过程和方式"><a href="#CI-CD的概念、执行过程和方式" class="headerlink" title="CI&#x2F;CD的概念、执行过程和方式"></a>CI&#x2F;CD的概念、执行过程和方式</h3><p>CI&#x2F;CD（持续集成&#x2F;持续交付）是一种软件开发实践，旨在通过自动化流程提高软件交付的速度和质量。</p>
<h4 id="CI（持续集成）"><a href="#CI（持续集成）" class="headerlink" title="CI（持续集成）"></a>CI（持续集成）</h4><ul>
<li>概念：开发人员频繁地将代码集成到共享仓库中</li>
<li>目的：尽早发现并解决集成问题，提高软件质量</li>
</ul>
<h4 id="CD（持续交付-部署）"><a href="#CD（持续交付-部署）" class="headerlink" title="CD（持续交付&#x2F;部署）"></a>CD（持续交付&#x2F;部署）</h4><ul>
<li>持续交付：自动化构建、测试，准备好随时可以部署到生产环境</li>
<li>持续部署：将持续交付的结果自动部署到生产环境</li>
</ul>
<h4 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h4><ol>
<li>代码提交：开发人员将代码提交到版本控制系统（如Git）</li>
<li>自动构建：CI服务器检测到代码变更，触发自动构建</li>
<li>单元测试：运行单元测试，确保新代码不破坏现有功能</li>
<li>代码分析：进行静态代码分析，检查代码质量</li>
<li>集成测试：运行集成测试，确保各模块正常协作</li>
<li>打包：将应用程序打包成可部署的格式</li>
<li>部署到测试环境：自动部署到测试或预生产环境</li>
<li>自动化测试：运行功能测试、性能测试等</li>
<li>审核和批准：（可选）人工审核测试结果</li>
<li>部署到生产：自动或手动触发部署到生产环境</li>
</ol>
<h4 id="执行方式"><a href="#执行方式" class="headerlink" title="执行方式"></a>执行方式</h4><ul>
<li>使用CI&#x2F;CD工具：如Jenkins、GitLab CI、Travis CI、CircleCI等</li>
<li>配置流水线：定义各个阶段的任务和顺序</li>
<li>触发机制：代码提交、定时任务或手动触发</li>
<li>通知机制：通过邮件、聊天工具等通知相关人员构建结果</li>
<li>版本控制：与Git等版本控制系统集成</li>
<li>容器化：使用Docker等容器技术确保环境一致性</li>
<li>基础设施即代码：使用Ansible、Terraform等工具管理基础设施</li>
</ul>
<p>CI&#x2F;CD的实施能够显著提高团队的开发效率，减少人为错误，加快产品迭代速度，是现代软件开发中不可或缺的一部分。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://unicorn-raya.github.io/2024/08/29/pupu-businesslogic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ray">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="摸鱼小据点">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/08/29/pupu-businesslogic/" class="post-title-link" itemprop="url">工作业务总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-08-29 10:47:13" itemprop="dateCreated datePublished" datetime="2024-08-29T10:47:13+08:00">2024-08-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-09-27 17:06:31" itemprop="dateModified" datetime="2024-09-27T17:06:31+08:00">2024-09-27</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="业务逻辑简介"><a href="#业务逻辑简介" class="headerlink" title="业务逻辑简介"></a>业务逻辑简介</h2><h3 id="整体囤货-直通逻辑"><a href="#整体囤货-直通逻辑" class="headerlink" title="整体囤货&#x2F;直通逻辑"></a>整体囤货&#x2F;直通逻辑</h3><p>首先，门店商品绑定订补货规则，订补货规则往往类似那种一周定三次，每次分别是哪些天，同时，规则中会有规则绑定的城市订单生成时间<br>门店商品绑定定补货规则之后，每天每个小时都会去订货规则表，清洗出符合这个时间段的订货规则以及门店<br>然后，符合订货规则的门店，会去抓供应商维度表和商品维度表，保证供应商合规或者商品合规。到这里，筛选的部分就完成了<br>接下来是计算流程，我们将城市商品分为总仓库存，总仓在途，门店售卖期商品<br>分三个阶段的目的是为了推算出订货规则中，绑定门店商品的售卖期的时间的期初库存，来计算为了满足售卖期的销量，需要多少的建议订货量。<br>然后门店的建议订货量会汇总成总仓、城市的建议订货量，最终按照商品-供应商来拆分城市订单</p>
<h3 id="整体逻辑中的模块"><a href="#整体逻辑中的模块" class="headerlink" title="整体逻辑中的模块"></a>整体逻辑中的模块</h3><h4 id="安全库存"><a href="#安全库存" class="headerlink" title="安全库存"></a>安全库存</h4><p>安全库存一共有11个指标，分别是</p>
<ul>
<li>10，20，30 历史7&#x2F;14&#x2F;31天的历史平均销量</li>
<li>50，60，70 未来7&#x2F;14&#x2F;31天的未来平均销量</li>
<li>80 售卖期平均销量</li>
<li>90&#x2F;10 售卖期延迟后的时间的销量：售卖期开始时间 + 延迟时间A ~ 售卖期结束时间 + 延迟时间B</li>
<li>90&#x2F;20 售卖期延迟后的时间的销量：售卖期结束时间 + 延迟时间A ~ 售卖期结束时间 + 延迟时间B</li>
<li>100 固定值</li>
<li>110 未来7天基础销量（同50~70）</li>
</ul>
<h5 id="80用例1："><a href="#80用例1：" class="headerlink" title="80用例1："></a>80用例1：</h5><p>门店商品售卖期 20240825 11：00 ~ 20240826 11：00,输出结果为两个整天的预测销量总和&#x2F;2</p>
<h5 id="80用例2："><a href="#80用例2：" class="headerlink" title="80用例2："></a>80用例2：</h5><p>门店商品售卖期 20240825 00：00 ~ 20240827 00：00,输出结果为三个整天的预测销量总和&#x2F;2</p>
<h5 id="90-10"><a href="#90-10" class="headerlink" title="90&#x2F;10"></a>90&#x2F;10</h5><p>门店商品售卖期 20240825 11：00 ~ 20240826 11：00,延迟时间为 2， 3<br>输出结果为  20240825 11 + 2：00 ~ 20240826 11 + 3：00 的预测销量之和</p>
<h5 id="回归用例"><a href="#回归用例" class="headerlink" title="回归用例"></a>回归用例</h5><p>取门店商品的历史销量和未来销量计算</p>
<h4 id="一品多供"><a href="#一品多供" class="headerlink" title="一品多供"></a>一品多供</h4><h5 id="一品多供分三种供应商分配规则"><a href="#一品多供分三种供应商分配规则" class="headerlink" title="一品多供分三种供应商分配规则"></a>一品多供分三种供应商分配规则</h5><ol>
<li>供应商按照比例来分配城市订单</li>
<li>供应商按照产能上限来分配城市订单</li>
<li>供应商按照门店组分配城市订单</li>
</ol>
<p>其中，直通的计算逻辑分三种:<br>直送门店，直通-定总仓，直通-定门店<br>直送门店在计算建议订货量的时候，就可以拆分<br>定总仓需要通过dc供应能力，汇总到总仓维度，然后再拆分。这里的计算逻辑和直送门店是一样的<br>定门店，由于需要先送到总仓，再送到门店。中间需要城市订货系数和门店补货系数的矫正，会产生差值，所以需要后续的差值分摊</p>
<h3 id="我是怎么测试的："><a href="#我是怎么测试的：" class="headerlink" title="我是怎么测试的："></a>我是怎么测试的：</h3><p>先按照业务场景，确定本次代码的改动范围，约定测试范围。除了主逻辑之外，还有总仓附录表<br>然后写用例：用例的计算流程用excel拉了中间表，然后用例评审，确定场景用例有效以及查缺补漏<br>执行用例：<br>自己造数据来满足需求对应的用例。<br>附录表只看字段兼容结果，以及最后去城市订单看前端展示结果</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://unicorn-raya.github.io/2021/11/24/rdd1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ray">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="摸鱼小据点">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/24/rdd1/" class="post-title-link" itemprop="url">论文存档</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-11-24 13:38:50" itemprop="dateCreated datePublished" datetime="2021-11-24T13:38:50+08:00">2021-11-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-08-29 11:07:32" itemprop="dateModified" datetime="2024-08-29T11:07:32+08:00">2024-08-29</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="读论文-RDD：一种基于内存的集群计算的容错性抽象方法"><a href="#读论文-RDD：一种基于内存的集群计算的容错性抽象方法" class="headerlink" title="读论文: RDD：一种基于内存的集群计算的容错性抽象方法"></a>读论文: RDD：一种基于内存的集群计算的容错性抽象方法</h1><p><a target="_blank" rel="noopener" href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-82.pdf">https://www2.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-82.pdf</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://unicorn-raya.github.io/2021/11/03/spark-read-file/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ray">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="摸鱼小据点">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/11/03/spark-read-file/" class="post-title-link" itemprop="url">如何用spark读取含有特殊字符的本地文件</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-11-03 16:08:39 / Modified: 18:22:56" itemprop="dateCreated datePublished" datetime="2021-11-03T16:08:39+08:00">2021-11-03</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近在项目上遇到需要读csv文件的情况，而且还是本地的。中间出现了数据读漏，读岔的情况。这里写一篇日志来总结我在读文件中遇到的坑。</p>
<p>比如一个csv文件，他的数据是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">name,age,score</span><br><span class="line">xiaoming,18,61</span><br><span class="line">xiaozhang,20,89</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>这种情况下往往我们都会用最常见的读取csv的操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sample_df = spark.read.format(&quot;csv&quot;).option(&quot;delimiter&quot;,&quot;,&quot;).option(&quot;header&quot;,True).load(LOCAL_FILE_PATH)</span><br><span class="line">sample_df.show(truncate = False)</span><br></pre></td></tr></table></figure>
<p>然而当数据变成以下情况时候，这种方法就失效了。</p>
<h2 id="Case-1-数据本身就有逗号"><a href="#Case-1-数据本身就有逗号" class="headerlink" title="Case 1 数据本身就有逗号"></a>Case 1 数据本身就有逗号</h2><p>同样是上述的数据，但是数据变成如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">name,age,score</span><br><span class="line">&quot;xiao,ming&quot;,18,61</span><br><span class="line">&quot;xiaozh,ang&quot;,20,89</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>此时仍然用上面的方法，数据会读成这样：</p>
<table>
<thead>
<tr>
<th>name</th>
<th>age</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>xiao</td>
<td>ming</td>
<td>18</td>
</tr>
<tr>
<td>xiaozh</td>
<td>ang</td>
<td>20</td>
</tr>
</tbody></table>
<p>这是因为当用了 option(“delimiter”,”,”) 的时候，每行数据都会用 “,” 隔开。这就会导致数据出现不正确的分割。要解决这种问题需要用如下的读取方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sample_df = spark.read.format(&quot;csv&quot;).option(&quot;quote&quot;, &quot;\&quot;&quot;).option(&#x27;escape&#x27;, &quot;\&quot;&quot;).option(&quot;header&quot;,True).load(LOCAL_FILE_PATH)</span><br><span class="line">sample_df.show(truncate = False)</span><br></pre></td></tr></table></figure>
<p>这样，上述的方法就可以读出正确的数据了。</p>
<h2 id="Case-2-数据含有回车"><a href="#Case-2-数据含有回车" class="headerlink" title="Case 2 数据含有回车"></a>Case 2 数据含有回车</h2><p>还有一种特殊情况，数据是这样的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">name,age,score</span><br><span class="line">&quot;xiao,m</span><br><span class="line">ing&quot;,18,61</span><br><span class="line">&quot;xiaozh,a</span><br><span class="line">ng&quot;,20,89</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>这种情况上述的方法会失效，上面的两种读取数据的方法都会得到以下的结果</p>
<table>
<thead>
<tr>
<th>name</th>
<th>age</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>xiao,m</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>ing</td>
<td>18</td>
<td>61</td>
</tr>
</tbody></table>
<p>在这种情况下，我们需要用如下的方法:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sample_df = spark.read.format(&quot;csv&quot;).option(&quot;quote&quot;, &quot;\&quot;&quot;).option(&#x27;escape&#x27;, &quot;\&quot;&quot;).option(&quot;multiline&quot;, True).option(&quot;header&quot;,True).load(LOCAL_FILE_PATH)</span><br><span class="line">sample_df.show(truncate = False)</span><br></pre></td></tr></table></figure>
<h2 id="一个通用解"><a href="#一个通用解" class="headerlink" title="一个通用解"></a>一个通用解</h2><p>如上两种方法可以解决绝大多数的特殊字符问题。然而，当数据本身就不是正规的csv文件时候，以上的两种方法仍然会失效。比如，数据是从一些crm系统导出，或者数据存储的编码类型可能会是gbk但是却存储成utf-8的情况(这里用excel打开会出现各种???,但是用<a target="_blank" rel="noopener" href="https://notepad-plus-plus.org/">notepad++</a>打开就能看到文件的真实样子)这时候，spark就会无能为力。<br>为了保证有一个统一的方案，这里推荐安装 <a target="_blank" rel="noopener" href="https://arrow.apache.org/docs/python/index.html">pyarrow</a> 以及<a target="_blank" rel="noopener" href="https://pandas.pydata.org/">pandas</a> 来读取本地的数据，然后将pandas的dataframe来转成spark的dataframe.<br>经过一些踩坑的处理，我得到最终的本地通用解是以下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession,DataFrame</span><br><span class="line">from pyspark.sql import functions as f</span><br><span class="line">from pyspark.sql.functions import when,udf,concat_ws,col,lit,concat,substring,regexp_replace,date_format,expr</span><br><span class="line">from pyspark.sql.types import DoubleType,IntegerType</span><br><span class="line">import pandas as pd</span><br><span class="line">pd.set_option(&quot;max_columns&quot;,10000)</span><br><span class="line">spark = SparkSession.builder.master(&#x27;local[*]&#x27;).appName(&#x27;sample&#x27;).config(&quot;spark.executor.memory&quot;, &quot;6g&quot;)\</span><br><span class="line">                                                .config(&quot;spark.driver.memory&quot;, &quot;5g&quot;)\</span><br><span class="line">                                                .config(&quot;spark.memory.offHeap.enabled&quot;,True)\</span><br><span class="line">                                                .config(&quot;spark.memory.offHeap.size&quot;,&quot;10g&quot;).getOrCreate()</span><br><span class="line"></span><br><span class="line">spark.sparkContext.setLogLevel(&quot;ERROR&quot;) </span><br><span class="line">spark.conf.set(&quot;spark.sql.execution.arrow.pyspark.enabled&quot;, &quot;true&quot;)</span><br><span class="line"></span><br><span class="line">def load_csv(spark:SparkSession, path:str, used_column:list = None) -&gt; DataFrame:</span><br><span class="line">    if used_column is None:</span><br><span class="line">        used_column = spark.read.format(&quot;csv&quot;).option(&quot;header&quot;,True).option(&quot;delimiter&quot;,&quot;,&quot;).load(path).columns</span><br><span class="line">    &quot;&quot;&quot;read csv to spark df&quot;&quot;&quot;</span><br><span class="line">    pd_df = pd.read_csv(path,dtype=str)[used_column]</span><br><span class="line">    pd_df = pd_df.where((pd.notnull(pd_df)), None)</span><br><span class="line">    df = spark.createDataFrame(pd_df.astype(str)).replace(&#x27;None&#x27;,None).replace(&#x27;&#x27;,None)</span><br><span class="line">    return df</span><br><span class="line">data = load_csv(spark,LOCAL_FILE_PATH)</span><br></pre></td></tr></table></figure>
<p>在用pyarrow的支持下，pandas的dataframe转spark的dataframe会快非常多。其中推荐首先选出需要的column,然后将列转到函数变量，而不是全量加载。此外函数内部同样处理了空值的情况，将None做了额外的处理。否则数据中的空值在pandas中会处理成NaN传递到spark的dataframe中，这种情况会导致df.isNull()&#x2F;df.isNotNull()判断失效。</p>
<h3 id="安装pyarrow的小坑"><a href="#安装pyarrow的小坑" class="headerlink" title="安装pyarrow的小坑"></a>安装pyarrow的小坑</h3><p>Q:执行安装命令后没有反应<br>A: 需要安装如下<a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/download/details.aspx?id=48145">依赖</a>，假如你的本地装了visual studio，本地会有一个vc2015-2019的依赖，这个依赖会和这两个依赖冲突，需要先卸载visual studio的依赖，再安装这个。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://unicorn-raya.github.io/2021/10/15/whyKafkaSoFast_cn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ray">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="摸鱼小据点">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/10/15/whyKafkaSoFast_cn/" class="post-title-link" itemprop="url">为什么kafka这么快（总结）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-10-15 12:49:35" itemprop="dateCreated datePublished" datetime="2021-10-15T12:49:35+08:00">2021-10-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-11-05 15:51:52" itemprop="dateModified" datetime="2021-11-05T15:51:52+08:00">2021-11-05</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h1><h2 id="文章一"><a href="#文章一" class="headerlink" title="文章一"></a>文章一</h2><h3 id="Broker-性能"><a href="#Broker-性能" class="headerlink" title="Broker 性能"></a>Broker 性能</h3><ul>
<li>日志结构的持久性</li>
<li>日志记录批处理</li>
<li>批量压缩</li>
<li>廉价的consumers</li>
<li>非强制刷新缓冲写操作</li>
<li>client 侧的优化</li>
<li>零拷贝</li>
<li>规避GC</li>
</ul>
<h3 id="流数据并行"><a href="#流数据并行" class="headerlink" title="流数据并行"></a>流数据并行</h3><h2 id="文章二"><a href="#文章二" class="headerlink" title="文章二"></a>文章二</h2><ul>
<li>利用partition实现并行处理</li>
<li>顺序写磁盘</li>
<li>充分利用page cache</li>
<li>零拷贝技术</li>
<li>网络数据持久化到磁盘(Producer到broker)</li>
<li>磁盘文件通过网络发送(broker到consumer)</li>
<li>批处理</li>
<li>数据压缩</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul>
<li>Partition 的并行处理</li>
<li>顺序写磁盘，充分利用磁盘的特性</li>
<li>利用page cache来利用内存提高i&#x2F;o 效率</li>
<li>零拷贝技术</li>
<li>producer生产的数据持久化到broker,猜用mmap文件映射，实现顺序的快速写入</li>
<li>customer从broker读取数据，采用sendfile，将磁盘文件读到os内核缓冲区之后，转到nio buffer进行网络发送，减少了cpu消耗</li>
</ul>
<hr>
<p><a target="_blank" rel="noopener" href="https://medium.com/swlh/why-kafka-is-so-fast-bde0d987cd03">原文链接1</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/146547962?utm_source=wechat_session">原文链接1翻译</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/183808742">原文链接2</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://unicorn-raya.github.io/2021/06/27/whyKafkaSoFast/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ray">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="摸鱼小据点">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/27/whyKafkaSoFast/" class="post-title-link" itemprop="url">Why kafka so fast</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-06-27 17:49:35" itemprop="dateCreated datePublished" datetime="2021-06-27T17:49:35+08:00">2021-06-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-09-27 21:52:56" itemprop="dateModified" datetime="2024-09-27T21:52:56+08:00">2024-09-27</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="https://medium.com/swlh/why-kafka-is-so-fast-bde0d987cd03">原文链接</a></p>
<h2 id="Discover-the-deliberate-design-decisions-that-have-made-Kafka-the-performance-powerhouse-it-is-today"><a href="#Discover-the-deliberate-design-decisions-that-have-made-Kafka-the-performance-powerhouse-it-is-today" class="headerlink" title="Discover the deliberate design decisions that have made Kafka the performance powerhouse it is today."></a>Discover the deliberate design decisions that have made Kafka the performance powerhouse it is today.</h2><p>The last few years have brought about immense changes in the software architecture landscape. The notion of a single monolithic application or even several coarse-grained services sharing a common data store has been all but erased from the hearts and minds of software practitioners world-wide. Autonomous microservices, event-driven architecture, and CQRS are the dominant tools in the construction of contemporary business-centric applications. To top it off, the proliferation of device connectivity — IoT, mobile, wearables — is creating an upward pressure on the number of events a system must handle in near-real-time.</p>
<p>Let’s start by acknowledging that the term ‘fast’ is multi-faceted, complex, and highly ambiguous. Latency, throughput, jitter, are metrics that shape and influence one’s interpretation of the term. It is also inherently contextual: the industry and application domains in themselves set the norms and expectations around performance. Whether or not something is fast depends largely on one’s frame of reference.</p>
<p>Apache Kafka is optimized for throughput at the expense of latency and jitter, while preserving other desirable qualities, such as durability, strict record order, and at-least-once delivery semantics. When someone says ‘Kafka is fast’, and assuming they are at least mildly competent, you can assume they are referring to Kafka’s ability to <em>safely</em> accumulate and distribute a very high number of records in a short amount of time.</p>
<p>Historically, Kafka was born out of LinkedIn’s need to move a very large number of messages efficiently, amounting to multiple terabytes of data on an hourly basis. The individual message propagation delay was deemed of secondary importance, as was the variability of that time. After all, LinkedIn is not a financial institution that engages in high-frequency trading, nor is it an industrial control system that operates within deterministic deadlines. Kafka can be used to implement near-real-time (otherwise known as soft real-time) systems.</p>
<blockquote>
<p><strong><em>Note:</em></strong> <em>For those unfamiliar with the term, ‘real-time’ does not mean ‘fast’, it means ‘predictable’. Specifically, real-time implies a hard upper bound, otherwise known as a</em> deadline_, on the time taken to complete an action. If the system as a whole is unable to meet this deadline each and every time, it cannot be classed as real-time. Systems that are able to perform within a probabilistic tolerance, are labelled as ‘near-real-time’. In terms of sheer throughput, real-time systems are often slower than their near-real-time or non-real-time counterparts._</p>
</blockquote>
<p>There are two significant areas that Kafka draws upon for its speed, and they need to be discussed separately. The first relates to the low-level efficiency of the client and broker implementations. The second derives from the opportunistic parallelism of stream processing.</p>
<h2 id="Log-structured-persistence"><a href="#Log-structured-persistence" class="headerlink" title="Log-structured persistence"></a>Log-structured persistence</h2><p>Kafka utilizes a segmented, <strong>append-only log</strong>, largely limiting itself to <strong>sequential I&#x2F;O</strong> for both reads and writes, which is fast across a wide variety of storage media. There is a wide misconception that disks are slow; however, the performance of storage media (particularly rotating media) is greatly dependent on access patterns. The performance of random I&#x2F;O on a typical 7,200 RPM SATA disk is between three and four orders of magnitude slower when compared to sequential I&#x2F;O. Furthermore, a modern operating system provides read-ahead and write-behind techniques that prefetch data in large block multiples and group smaller logical writes into large physical writes. Because of this, the difference between sequential I&#x2F;O and random I&#x2F;O is still evident in flash and other forms of solid-state non-volatile media, although it is far less dramatic compared to rotating media.</p>
<h2 id="Record-batching"><a href="#Record-batching" class="headerlink" title="Record batching"></a>Record batching</h2><p>Sequential I&#x2F;O is blazingly fast on most media types, comparable to the peak performance of network I&#x2F;O. In practice, this means that a well-designed log-structured persistence layer will keep up with the network traffic. In fact, quite often the bottleneck with Kafka isn’t the disk, but the network. So in addition to the low-level batching provided by the OS, Kafka clients and brokers will accumulate multiple records in a batch — for both reading and writing — before sending them over the network. Batching of records amortizes the overhead of the network round-trip, using larger packets and improving bandwidth efficiency.</p>
<h2 id="Batch-compression"><a href="#Batch-compression" class="headerlink" title="Batch compression"></a>Batch compression</h2><p>The impact of batching is particularly obvious when compression is enabled, as compression becomes generally more effective as the data size increases. Especially when using text-based formats such as JSON, the effects of compression can be quite pronounced, with compression ratios typically ranging from 5x to 7x. Furthermore, record batching is largely done as a client-side operation, which transfers the load onto the client and has a positive effect not only on the network bandwidth but also on the brokers’ disk I&#x2F;O utilization.</p>
<h2 id="Cheap-consumers"><a href="#Cheap-consumers" class="headerlink" title="Cheap consumers"></a>Cheap consumers</h2><p>Unlike traditional MQ-style brokers which remove messages at point of consumption (incurring the penalty of random I&#x2F;O), Kafka doesn’t remove messages after they are consumed — instead, it independently tracks offsets at each consumer group level. The progression of offsets themselves is published on an internal Kafka topic <code>__consumer_offsets</code>. Again, being an append-only operation, this is fast. The contents of this topic are further reduced in the background (using Kafka’s compaction feature) to only retain the last known offsets for any given consumer group.</p>
<p>Compare this model with more traditional message brokers which typically offer several contrasting message distribution topologies. On one hand is the message queue — a durable transport for point-to-point messaging, with no point-to-multipoint ability. On the other hand, a pub-sub topic allows for point-to-multipoint messaging but does so at the expense of durability. Implementing a durable point-to-multipoint messaging model in a traditional MQ requires maintaining a dedicated message queue for each stateful consumer. This creates both read and write amplification. On one hand, the publisher is forced to write to multiple queues. Alternatively, a fan-out relay may consume records from one queue and write to several others, but this only defers the point of amplification. On the other hand, several consumers are generating load on the broker — being a mixture of read and write I&#x2F;O, both sequential and random.</p>
<p>Consumers in Kafka are ‘cheap’, insofar as they don’t mutate the log files (only the producer or internal Kafka processes are permitted to do that). This means that a large number of consumers may concurrently read from the same topic without overwhelming the cluster. There is still some cost in adding a consumer, but it is mostly sequential reads with a low rate of sequential writes. So it’s fairly normal to see a single topic being shared across a diverse consumer ecosystem.</p>
<h2 id="Unflushed-buffered-writes"><a href="#Unflushed-buffered-writes" class="headerlink" title="Unflushed buffered writes"></a>Unflushed buffered writes</h2><p>Another fundamental reason for Kafka’s performance, and one that is worth exploring further: Kafka doesn’t actually call <code>fsync</code> when writing to the disk before acknowledging the write; the only requirement for an ACK is that the record has been written to the I&#x2F;O buffer. This is a little known fact, but a crucial one: in fact, this is what actually makes Kafka perform as if it were an in-memory queue — because for all intents and purposes <em>Kafka is a disk-backed in-memory queue</em> (limited by the size of the buffer&#x2F;pagecache).</p>
<p>On the flip side, this form of writing is unsafe, as the failure of a replica can lead to a data loss even though the record has seemingly been acknowledged. In other words, unlike say a relational database, acknowledging a write alone does not imply durability. What makes Kafka durable is running several in-sync replicas; even if one were to fail, the others (assuming there is more than one) will remain operational — providing that the failure is uncorrelated (i.e. multiple replicas failing simultaneously due of a common upstream failure). So the combination of a non-blocking approach to I&#x2F;O with no <code>fsync</code>, and redundant in-sync replicas give Kafka the combination of high throughput, durability, and availability.</p>
<h2 id="Client-side-optimisations"><a href="#Client-side-optimisations" class="headerlink" title="Client-side optimisations"></a>Client-side optimisations</h2><p>Most databases, queues, and other forms of persistent middleware are designed around the notion of an all-mighty server (or a cluster of servers), and fairly thin clients that communicate with the server(s) over a well-known wire protocol. Client implementations are generally considered to be significantly simpler than the server. As a result, the server will absorb the bulk of the load — the clients merely act as interfaces between the application code and the server.</p>
<p>Kafka takes a different approach to client design. A significant amount of work is performed on the client before records get to the server. This includes the staging of records in an accumulator, hashing the record keys to arrive at the correct partition index, checksumming the records and the compression of the record batch. The client is aware of the cluster metadata and periodically refreshes this metadata to keep abreast of any changes to the broker topology. This lets the client make low-level forwarding decisions; rather than sending a record blindly to the cluster and relying on the latter to forward it to the appropriate broker node, a producer client will forward writes directly to partition masters. Similarly, consumer clients are able to make intelligent decisions when sourcing records, potentially using replicas that geographically closer to the client when issuing read queries. (This feature is a more recent addition to Kafka, available as of version 2.4.0.)</p>
<h2 id="Zero-copy"><a href="#Zero-copy" class="headerlink" title="Zero-copy"></a>Zero-copy</h2><p>One of the typical sources of inefficiencies is copying byte data between buffers. Kafka uses a binary message format that is shared by the producer, the broker, and the consumer parties so that data chunks can flow end-to-end without modification, even if it’s compressed. While eliminating structural differences between communicating parties is an important step, it doesn’t in itself avoid the copying of data.</p>
<p>Kafka solves this problem on Linux and UNIX systems by using Java’s NIO framework, specifically, the <code>transferTo()</code> method of a <code>java.nio.channels.FileChannel</code>. This method permits the transfer of bytes from a source channel to a sink channel without involving the application as a transfer intermediary. To appreciate the difference that NIO makes, consider the traditional approach where a source channel is read into a byte buffer, then written to a sink channel as two separate operations:</p>
<p>File.read(fileDesc, buf, len);<br>Socket.send(socket, buf, len);</p>
<p>Diagrammatically, this can be represented using the following.</p>
<p><img src="/2021/06/27/whyKafkaSoFast/p1.gif" alt="p1"></p>
<!-- <center><img src="kafka/p1.gif" width = "500" height = "500"></center> -->


<p>Although this looks simple enough, internally, the copy operation requires four context switches between user mode and kernel mode, and the data is copied four times before the operation is complete. The diagram below outlines the context switches at each step.</p>
<p><img src="/2021/06/27/whyKafkaSoFast/p2.gif" alt="p2"></p>
<p>Looking at this in more detail —</p>
<ol>
<li>The initial <code>read()</code> causes a context switch from user mode to kernel mode. The file is read, and its contents are copied to a buffer in the kernel address space by the DMA (Direct Memory Access) engine. This is <em>not</em> the same buffer that was used in the code snippet.</li>
<li>Prior to returning from <code>read()</code>, the kernel buffer is copied into the user-space buffer. At this point, our application can read the contents of the file.</li>
<li>The subsequent <code>send()</code> will switch back into kernel mode, copying the user-space buffer into the kernel address space — this time into a different buffer associated with the destination socket. Behind the scenes, the DMA engine takes over, asynchronously copying the data from the kernel buffer to the protocol stack. The <code>send()</code> method does not wait for this prior to returning.</li>
<li>The <code>send()</code> call returns, switching back to the user-space context.</li>
</ol>
<p>In spite of its mode-switching inefficiencies and additional copying, the intermediate kernel buffer can actually improve performance in many cases. It can act as a read-ahead cache, asynchronously prefetching blocks, thereby front-running requests from the application. However, when the amount of requested data is significantly larger than the kernel buffer size, the kernel buffer becomes a performance bottleneck. Rather than copying the data directly, it forces the system to oscillate between user and kernel modes until all the data is transferred.</p>
<p>By contrast, the zero-copy approach is handled in a single operation. The snippet from the earlier example can be rewritten as a one-liner:</p>
<p>fileDesc.transferTo(offset, len, socket);</p>
<p>The zero-copy approach is illustrated below.</p>
<p><img src="/2021/06/27/whyKafkaSoFast/p3.gif" alt="p3"></p>
<p>Under this model, the number of context switches is reduced to one. Specifically, the <code>transferTo()</code> method instructs the block device to read data into a read buffer by the DMA engine. This buffer is then copied another kernel buffer for staging to the socket. Finally, the socket buffer is copied to the NIC buffer by DMA.</p>
<p><img src="/2021/06/27/whyKafkaSoFast/p4.gif" alt="p4"></p>
<p>As a result, we have reduced the number of copies from four to three, and only one of those copies involves the CPU. We have also reduced the number of context switches from four to two.</p>
<p>This is a massive improvement, but it’s not query zero-copy yet. The latter can be achieved as a further optimization when running Linux kernels 2.4 and later, and on network interface cards that support the <code>gather</code> operation. This is illustrated below.</p>
<p><img src="/2021/06/27/whyKafkaSoFast/p5.gif" alt="p5"></p>
<p>Calling the <code>transferTo()</code> method causes the device to read data into a kernel read buffer by the DMA engine, as per the previous example. However, with the <code>gather</code> operation, there is no copying between the read buffer and the socket buffer. Instead, the NIC is given a pointer to the read buffer, along with the offset and the length, which is vacuumed up by DMA. At no point is the CPU involved in copying buffers.</p>
<p>Comparisons of traditional and zero-copy on file sizes ranging from a few megabytes to a gigabyte show performance gains by a factor of two to three in favor of zero-copy. But what’s more impressive, is that Kafka achieves this using a plain JVM with no native libraries or JNI code.</p>
<h2 id="Avoiding-the-GC"><a href="#Avoiding-the-GC" class="headerlink" title="Avoiding the GC"></a>Avoiding the GC</h2><p>The heavy use of channels, native buffers, and the page cache has one additional benefit — reducing the load on the garbage collector (GC). For example, running Kafka on a machine with 32 GB of RAM will result in 28–30 GB usable for the page cache, completely outside of the GC’s scope. The difference in throughput is minimal — in the region of several percentage points — as the throughput of a correctly-tuned GC can be quite high, especially when dealing with short-lived objects. The real gains are in the reduction of jitter; by avoiding the GC, the brokers are less likely to experience a pause that may impact the client, extending the end-to-end propagation delay of records.</p>
<p>To be fair, the avoidance of GC is less of a problem now, compared to what it used to be when Kafka was conceived. Modern GCs like Shenandoah and ZGC scale to huge, multi-terabyte heaps, and have tunable worst-case pause times, down to single-digit milliseconds. It is not uncommon these days to see JVM-based applications using large heap-based caches outperform off-heap designs.</p>
<p>The efficiency of log-structured I&#x2F;O is one crucial aspect of performance, mostly affecting writes; Kafka’s treatment of parallelism in the topic structure and the consumer ecosystem is fundamental to its read performance. The combination produces an overall very high end-to-end messaging throughput. Concurrency is ingrained into its partitioning scheme and the operation of consumer groups, which is effectively a load-balancing mechanism within Kafka — distributing partition assignments approximately evenly among the individual consumer instances within the group. Compare this to a more traditional MQ: in an equivalent RabbitMQ setup, multiple concurrent consumers may read from a queue in a round-robin fashion, but in doing so they forfeit the notion of message ordering.</p>
<p>The partitioning mechanism also allows for the horizontal scalability of Kafka brokers. Every partition has a dedicated leader; any nontrivial topic (with multiple partitions) can, therefore, utilize the entire cluster of broker for writes. This is yet another point of distinction between Kafka and a message queue; where the latter utilizes clustering for availability, Kafka will genuinely balance the load across the brokers for availability, durability, and throughput.</p>
<p>The producer specifies the partition when publishing a record, assuming that you are publishing to a topic with multiple partitions. (One may have a single-partition topic, in which case this is a non-issue.) This may be accomplished either directly — by specifying a partition index, or indirectly — by way of a record key, which deterministically hashes to a consistent (i.e. same every time) partition index. Records sharing the same hash are guaranteed to occupy the same partition. Assuming a topic with multiple partitions, records with a different key will likely end up in different partitions. However, due to hash collisions, records with different hashes may also end up in the same partition. Such is the nature of hashing. If you understand how a hash table works, this is no different.</p>
<p>The actual processing of records is done by consumers, operating within an (optional) consumer group. Kafka guarantees that a partition may only be assigned to at most one consumer within its consumer group. (We say <em>‘at most’</em> to cover the case when all consumers are offline.) When the first consumer in a group subscribes to the topic, it will receive all partitions on that topic. When a second consumer subsequently joins, it will get approximately half of the partitions, relieving the first consumer of half of its prior load. This enables you to process an event stream in parallel, adding consumers as necessary (ideally, using an auto-scaling mechanism), providing that you have adequately partitioned your event stream.</p>
<p>Control of record throughput accomplished in two ways:</p>
<ol>
<li><strong>The topic partitioning scheme.</strong> Topics should be partitioned to maximize the number of independent event sub-streams. In other words, record order should only be preserved where it is <em>absolutely necessary</em>. If any two records are not legitimately related in a causal sense, they shouldn’t be bound to the same partition. This implies the use of different keys, as Kafka will use a record’s key as a hashing source to derive its consistent partition mapping.</li>
<li><strong>The number of consumers in the group.</strong> You can increase the number of consumers to match the load of inbound records, up to the number of partitions in the topic. (You can have more consumers if you wish, but the partition count will place an upper bound on the number of <em>active</em> consumers which get at least one partition assignment; the remaining consumers will remain idle.) Note that a consumer could be a process or a thread. Depending on the type of workload that the consumer performs, you may be able to employ multiple individual consumer threads or process records in a thread pool.</li>
</ol>
<p>If you were wondering whether Kafka is fast, how it achieves its renowned performance characteristics, or if it can scale to your use cases, you should hopefully by now have all the answers you need.</p>
<p>To make things abundantly clear, Kafka is not the fastest (that is, most throughput-capable) messaging middleware — there are other platforms capable of greater throughput — some are software-based and some are implemented in hardware. Nor is it the best throughput-latency compromise — <a target="_blank" rel="noopener" href="https://pulsar.apache.org/">Apache Pulsar</a> is a promising technology that is scalable and achieves a better throughput-latency profile while offering identical ordering and durability guarantees. The rationale for adopting Kafka is that as a complete ecosystem, it remains unmatched overall. It exhibits excellent performance while offering an environment that is abundant and mature, but also involving — in spite of its size, Kafka is still growing at an enviable pace.</p>
<p>The designers and maintainers of Kafka have done an amazing job at devising a solution that is performance-oriented at its core. Few of its design elements feel like an afterthought or a bolt-on. From offloading of work to clients to the log-structured persistence on the broker, batching, compression, zero-copy I&#x2F;O, and stream-level parallelism — Kafka throws down the gauntlet to just about any other message-oriented middleware, commercial or open-source. And most impressively, it does so without compromising on qualities such as durability, record order, and at-least-once delivery semantics.</p>
<p>Kafka is not the simplest of messaging platforms, and there is a fair bit to learn. One must come to grips with the concepts of a total and partial order, topics, partitions, consumers and consumer groups, before comfortably designing and building high-performance event-driven systems. And while the knowledge curve is substantial, the results will certainly be worth your while. If you are keen on taking the proverbial ‘red pill’, read the <a target="_blank" rel="noopener" href="https://medium.com/swlh/introduction-to-event-streaming-with-kafka-and-kafdrop-22afdb4b380a">Introduction to Event Streaming with Kafka and Kafdrop</a>.</p>
<p><em>Was this article useful to you? I’d love to hear your feedback, so don’t hold back. If you are interested in Kafka, Kubernetes, microservices, or event streaming, or just have any questions,</em> <a target="_blank" rel="noopener" href="https://twitter.com/i/user/562466177"><em>follow me on Twitter</em></a><em>. I’m also a maintainer of</em> <a target="_blank" rel="noopener" href="https://github.com/obsidiandynamics/kafdrop"><em>Kafdrop</em></a> <em>and the author of</em> <a target="_blank" rel="noopener" href="https://www.apachekafkabook.com/"><em>Effective Kafka</em></a><em>.</em></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://unicorn-raya.github.io/2021/06/01/leetcode%E5%B8%B8%E8%A7%81%E8%AF%AD%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ray">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="摸鱼小据点">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/01/leetcode%E5%B8%B8%E8%A7%81%E8%AF%AD%E6%B3%95/" class="post-title-link" itemprop="url">leetcode 常用语法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-06-01 13:08:39" itemprop="dateCreated datePublished" datetime="2021-06-01T13:08:39+08:00">2021-06-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-11-03 21:34:56" itemprop="dateModified" datetime="2021-11-03T21:34:56+08:00">2021-11-03</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>leetcode的刷题策略按照<a target="_blank" rel="noopener" href="https://github.com/Unicorn-raya/leetcode-master">这里</a>的顺序.这里整理一下java刷题会用到的基础语法</p>
<h1 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h1><h2 id="java"><a href="#java" class="headerlink" title="java"></a>java</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int n = 3, m = 4;</span><br><span class="line">int[] m_array = new int[n];</span><br><span class="line">int[][] m_2D_array = new int[n][m];</span><br><span class="line">// 由于java自带的垃圾回收，所以不需要释放内存 </span><br></pre></td></tr></table></figure>

<h2 id="c"><a href="#c" class="headerlink" title="c++"></a>c++</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;vector&gt;</span><br><span class="line"></span><br><span class="line">vector&lt;int&gt; m_vector;</span><br><span class="line">// 排序</span><br><span class="line">sort(m_vector.begin(), m_vector.end());</span><br><span class="line">// 取数组大小</span><br><span class="line">m_vector.size()      </span><br></pre></td></tr></table></figure>
<h2 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h2><h1 id="Hash表"><a href="#Hash表" class="headerlink" title="Hash表"></a>Hash表</h1><h2 id="c-1"><a href="#c-1" class="headerlink" title="c++"></a>c++</h2><p>c++ 中有三种数据结构可以用来处理hash表的问题，除了数组，还有set（集合）和map(映射)，下表是优劣对比   </p>
<h3 id="集合"><a href="#集合" class="headerlink" title="集合:"></a>集合:</h3><table>
<thead>
<tr>
<th>集合</th>
<th>底层实现</th>
<th>是否有序</th>
<th>数值是否可以重复</th>
<th>能否更改数值</th>
<th>查询效率</th>
<th>增删效率</th>
</tr>
</thead>
<tbody><tr>
<td>std::set</td>
<td>红黑树</td>
<td>有序</td>
<td>否</td>
<td>否</td>
<td>O(logn)</td>
<td>O(logn)</td>
</tr>
<tr>
<td>std::multiset</td>
<td>红黑树</td>
<td>有序</td>
<td>是</td>
<td>否</td>
<td>O(logn)</td>
<td>O(logn)</td>
</tr>
<tr>
<td>std::unordered_set</td>
<td>哈希表</td>
<td>无序</td>
<td>否</td>
<td>否</td>
<td>O(1)</td>
<td>O(1)</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 初始化</span><br><span class="line">unordered_set&lt;int&gt; s;</span><br><span class="line">unordered_set&lt;int&gt; s(num.begin(),num.end());</span><br><span class="line">//搜索</span><br><span class="line">s.find(num) // 返回的是num在s中的index</span><br><span class="line">s.insert(num) // 插入num</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>std::unordered_set底层实现为哈希表，std::set 和std::multiset 的底层实现是红黑树，红黑树是一种平衡二叉搜索树，所以key值是有序的，但key不可以修改，改动key值会导致整棵树的错乱，所以只能删除和增加。</p>
<table>
<thead>
<tr>
<th>映射</th>
<th>底层实现</th>
<th>是否有序</th>
<th>数值是否可以重复</th>
<th>能否更改数值</th>
<th>查询效率</th>
<th>增删效率</th>
</tr>
</thead>
<tbody><tr>
<td>std::map</td>
<td>红黑树</td>
<td>key有序</td>
<td>key不可重复</td>
<td>key不可修改</td>
<td>O(logn)</td>
<td>O(logn)</td>
</tr>
<tr>
<td>std::multimap</td>
<td>红黑树</td>
<td>key有序</td>
<td>key可重复</td>
<td>key不可修改</td>
<td>O(logn)</td>
<td>O(logn)</td>
</tr>
<tr>
<td>std::unordered_map</td>
<td>哈希表</td>
<td>key无序</td>
<td>key不可重复</td>
<td>key不可修改</td>
<td>O(1)</td>
<td>O(1)</td>
</tr>
</tbody></table>
<h3 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h3><h2 id="Java-1"><a href="#Java-1" class="headerlink" title="Java"></a>Java</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://unicorn-raya.github.io/2021/05/26/hive-advanced/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ray">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="摸鱼小据点">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/05/26/hive-advanced/" class="post-title-link" itemprop="url">Hive的高级特性</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-05-26 16:15:21" itemprop="dateCreated datePublished" datetime="2021-05-26T16:15:21+08:00">2021-05-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-05-27 18:07:08" itemprop="dateModified" datetime="2021-05-27T18:07:08+08:00">2021-05-27</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Hive的高级特性"><a href="#Hive的高级特性" class="headerlink" title="Hive的高级特性"></a>Hive的高级特性</h1><h2 id="Hive-UDF-UDAF-UDTF"><a href="#Hive-UDF-UDAF-UDTF" class="headerlink" title="Hive UDF,UDAF,UDTF"></a>Hive UDF,UDAF,UDTF</h2><p>Hive 的三种自定义function: UDF,UDAF,UDTF。详细见<a target="_blank" rel="noopener" href="https://blog.csdn.net/u010376788/article/details/50532166">链接</a></p>
<h2 id="Hive-incremental-updates"><a href="#Hive-incremental-updates" class="headerlink" title="Hive incremental updates"></a>Hive incremental updates</h2><h3 id="无事务类型定义之前的Hive"><a href="#无事务类型定义之前的Hive" class="headerlink" title="无事务类型定义之前的Hive"></a>无事务类型定义之前的Hive</h3><p>大数据领域中，大部分的表更新分为两种，full load和incremental update。在rdbms中，这两种操作都可以做到，但是在hive中，由于hive不是非关系型数据库，所以需要一些额外的操作:</p>
<h3 id="concepts"><a href="#concepts" class="headerlink" title="concepts"></a>concepts</h3><ul>
<li>master table：internal table,存储主数据</li>
<li>delta table： external table，里面只有更新的数据，或者需要增加的数据</li>
<li>reconciliation view: 通过master table和 delta table一起union all得到的结果</li>
<li>reporting table: snapshot of the reconciliation view. table的数据会随着delta table变化，通常会做一个新表来存数据<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table report_table as select * from reconciliation_view</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><p>分两种情况： master table 存在和不存在:<br>假如master table 不存在, 就需要：</p>
<ul>
<li>把数据导入到hdfs</li>
<li>做一个internal master table</li>
<li>搞定，这样就有master table了</li>
</ul>
<p>当master table存在时:</p>
<ul>
<li>先导入updated data 到 hdfs (new data)</li>
<li>hive 上建立一个external dalta table，路径指向new data的路径</li>
<li>建立一个 reconciliation view， 数据是 union master table 和 delta table的结果</li>
<li>删除掉detla table的hdfs上的数据</li>
<li>重建一个master table(purge)，然后从report table 导入数据</li>
</ul>
<p>一例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">select delta_1.* from </span><br><span class="line">(</span><br><span class="line">    select * from master_table</span><br><span class="line">    union</span><br><span class="line">    select * from delta_table</span><br><span class="line">) delta_1</span><br><span class="line">join </span><br><span class="line">(</span><br><span class="line">    select id,max(timestamp) as max_timestamp</span><br><span class="line">    from (</span><br><span class="line">        select * from master_table</span><br><span class="line">        union</span><br><span class="line">        select * from delta_table</span><br><span class="line">    )group by id</span><br><span class="line">)delta_2</span><br><span class="line">on delta_1.id = delta_2.id and delta_1.timestamp = delta2.max_timestamp</span><br><span class="line">and delta_1.delete_data is null</span><br></pre></td></tr></table></figure>
<h3 id="New-MERGE-statement-in-Hive-2-2"><a href="#New-MERGE-statement-in-Hive-2-2" class="headerlink" title="New MERGE statement in Hive 2.2"></a>New MERGE statement in Hive 2.2</h3><p>Hive 2.2之后，支持了merge的操作。但是需要master table 是 transactional.merge 的写法如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">merge into master</span><br><span class="line"> using delta on master.id = delta.id</span><br><span class="line"> when matched then update set</span><br><span class="line">   country=delta.country,</span><br><span class="line">   state=delta.state</span><br><span class="line">when matched and delete_date is not NULL then delete</span><br><span class="line">when not matched then insert</span><br><span class="line">   values(delta.id, delta.country, delta.state);</span><br></pre></td></tr></table></figure>
<p>这里，hive支持acid操作，但是update,delete,merge都只能在transactional table上有效</p>
<h4 id="Hive-ACID-事务表-transactions"><a href="#Hive-ACID-事务表-transactions" class="headerlink" title="Hive ACID 事务表(transactions)"></a>Hive ACID 事务表(transactions)</h4><p>限制:    </p>
<ul>
<li>需要全局支持</li>
<li>需要设置属性: tblproperties(“transactional”&#x3D;“true”)</li>
<li>bucketed(分桶，但是不一定要分区), internal table（Hive 管理）</li>
<li>数据是orc 格式</li>
</ul>
<p>一例:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE transactional_table (id int, username string)</span><br><span class="line">PARTITIONED BY (timestamp_date date)</span><br><span class="line">CLUSTERED BY(username) INTO 3 BUCKETS</span><br><span class="line">STORED AS ORC TBLPROPERTIES (&#x27;transactional&#x27;=&#x27;true&#x27;);</span><br></pre></td></tr></table></figure>
<p>一旦确定了hive表是tranaction类型，就无法再更改。且表不支持BEGIN,COMMIT,ROLLBACK语句。默认自动abort，或者手动abort.<br>并发属于乐观并发，先commit先win.<br>尽量用大规模数据更新，小规模数据可以用hive stream api.<br>由于HDFS不支持修改源文件，所以hive更新时候，会创建delta table，为了让增量文件尽量小，这里会有一个压缩的过程。定期合并增量文件，以及基础文件</p>
<h4 id="Hive-ACID-transactions-design"><a href="#Hive-ACID-transactions-design" class="headerlink" title="Hive ACID transactions design"></a>Hive ACID transactions design</h4><p>hive中的事务表过程一共有四个组件:</p>
<ul>
<li>Base file：表数据的主文件  </li>
<li>Delta files: 0或者多个delta file，在transaction过程(update,deleta)中创建</li>
<li>Compactor: 主要用来减少delta file。</li>
<li>Transaction&#x2F;Lock manager: 管理hive metastore 中的事务锁，事务和锁用来恢复失败transaction。transaction manager能保证open,commit,abort的状态。lock manager来保证事务表的必要的locks</li>
</ul>
<p>Delta 文件的压缩:<br>Compactor是一个在Hive Metastore上运行的一系列后台线程，主要包括Initiator, Worker, Cleaner, AcidHouseKeeperService 以及一些其他的组件。</p>
<ul>
<li>minor 压缩：将多个delta文件合并成一个delta文件 （维度是分桶级别）</li>
<li>major 压缩：将多个delta文件和base文件合并成一个新的base文件 （维度是分桶级别）</li>
</ul>
<p>用来监控事务的几个command：</p>
<ul>
<li>SHOW TRANSACTIONS </li>
<li>ABORT TRANSACTIONS </li>
<li>SHOW LOCKS </li>
<li>SHOW COMPACTIONS</li>
</ul>
<p>更加详细的看: <a target="_blank" rel="noopener" href="http://shzhangji.com/cnblogs/2019/06/11/understanding-hive-acid-transactional-table/">深入理解 Hive ACID 事务表</a> 以及 <a target="_blank" rel="noopener" href="https://blog.csdn.net/DataIntel_XiAn/article/details/102780436">Hive Merge详解</a></p>
<h2 id="Hive-streaming-API"><a href="#Hive-streaming-API" class="headerlink" title="Hive streaming API"></a>Hive streaming API</h2><p>更多应用看<a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/777720">这篇</a></p>
<h2 id="Hive-statistics"><a href="#Hive-statistics" class="headerlink" title="Hive statistics"></a>Hive statistics</h2><p>见<a target="_blank" rel="noopener" href="https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference/StatsDev.html">文档</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://unicorn-raya.github.io/2021/05/25/sort/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ray">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="摸鱼小据点">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/05/25/sort/" class="post-title-link" itemprop="url">常见排序算法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-05-25 10:42:15" itemprop="dateCreated datePublished" datetime="2021-05-25T10:42:15+08:00">2021-05-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-05-26 16:13:51" itemprop="dateModified" datetime="2021-05-26T16:13:51+08:00">2021-05-26</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="基础排序算法"><a href="#基础排序算法" class="headerlink" title="基础排序算法"></a>基础排序算法</h1><h2 id="快速排序-Quick-sort"><a href="#快速排序-Quick-sort" class="headerlink" title="快速排序(Quick sort)"></a>快速排序(Quick sort)</h2><p>思路就是分治，先找一个key，做一轮排序后，让key左边的数字都小于等于key,右边的大于key。在对半继续做这种排序</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">private void quickSort(int[] src, int begin, int end) &#123;</span><br><span class="line">    if (begin &lt; end) &#123;</span><br><span class="line">        int key = src[begin];</span><br><span class="line">        int i = begin;</span><br><span class="line">        int j = end;</span><br><span class="line">        while (i &lt; j) &#123;</span><br><span class="line">            while (i &lt; j &amp;&amp; src[j] &gt; key) &#123;</span><br><span class="line">                j--;</span><br><span class="line">            &#125;</span><br><span class="line">            if (i &lt; j) &#123;</span><br><span class="line">                int tmp = src[i];</span><br><span class="line">                src[i] = src[j];</span><br><span class="line">                src[j] = tmp;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            while (i &lt; j &amp;&amp; src[i] &lt; key) &#123;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            if (i &lt; j) &#123;</span><br><span class="line">                int tmp = src[i];</span><br><span class="line">                src[i] = src[j];</span><br><span class="line">                src[j] = tmp;</span><br><span class="line">                j--;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        //src[i] = key;</span><br><span class="line">        quickSort(src, begin, i - 1);</span><br><span class="line">        quickSort(src, i + 1, end);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>详细图解: <a target="_blank" rel="noopener" href="http://data.biancheng.net/view/117.html">http://data.biancheng.net/view/117.html</a> q</p>
<h2 id="堆排序-heap-sort"><a href="#堆排序-heap-sort" class="headerlink" title="堆排序 (heap sort)"></a>堆排序 (heap sort)</h2><p>堆(heap) 是一种完全二叉树，分为两种，大顶堆和小顶堆，满足以下关系:    </p>
<ul>
<li>大顶堆: arr[i] &gt;&#x3D; arr[2i+1] &amp;&amp; arr[i] &gt;&#x3D; arr[2i+2]</li>
<li>小顶堆: arr[i] &lt;&#x3D; arr[2i+1] &amp;&amp; arr[i] &lt;&#x3D; arr[2i+2]<br>通常升序用大顶堆，降序用小顶堆。<br>基本思路: </li>
<li>将无序数组变成大顶堆</li>
<li>将首个数值换到最后一个</li>
<li>将剩下的继续调整成大顶堆<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">package sortdemo;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Created by chengxiao on 2016/12/17.</span><br><span class="line"> * 堆排序demo</span><br><span class="line"> */</span><br><span class="line">public class HeapSort &#123;</span><br><span class="line">    public static void main(String []args)&#123;</span><br><span class="line">        int []arr = &#123;9,8,7,6,5,4,3,2,1&#125;;</span><br><span class="line">        sort(arr);</span><br><span class="line">        System.out.println(Arrays.toString(arr));</span><br><span class="line">    &#125;</span><br><span class="line">    public static void sort(int []arr)&#123;</span><br><span class="line">        //1.构建大顶堆</span><br><span class="line">        for(int i=arr.length/2-1;i&gt;=0;i--)&#123;</span><br><span class="line">            //从第一个非叶子结点从下至上，从右至左调整结构</span><br><span class="line">            adjustHeap(arr,i,arr.length);</span><br><span class="line">        &#125;</span><br><span class="line">        //2.调整堆结构+交换堆顶元素与末尾元素</span><br><span class="line">        for(int j=arr.length-1;j&gt;0;j--)&#123;</span><br><span class="line">            swap(arr,0,j);//将堆顶元素与末尾元素进行交换</span><br><span class="line">            adjustHeap(arr,0,j);//重新对堆进行调整</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 调整大顶堆（仅是调整过程，建立在大顶堆已构建的基础上）</span><br><span class="line">     * @param arr</span><br><span class="line">     * @param i</span><br><span class="line">     * @param length</span><br><span class="line">     */</span><br><span class="line">    public static void adjustHeap(int []arr,int i,int length)&#123;</span><br><span class="line">        int temp = arr[i];//先取出当前元素i</span><br><span class="line">        for(int k=i*2+1;k&lt;length;k=k*2+1)&#123;//从i结点的左子结点开始，也就是2i+1处开始</span><br><span class="line">            if(k+1&lt;length &amp;&amp; arr[k]&lt;arr[k+1])&#123;//如果左子结点小于右子结点，k指向右子结点</span><br><span class="line">                k++;</span><br><span class="line">            &#125;</span><br><span class="line">            if(arr[k] &gt;temp)&#123;//如果子节点大于父节点，将子节点值赋给父节点（不用进行交换）</span><br><span class="line">                arr[i] = arr[k];</span><br><span class="line">                i = k;</span><br><span class="line">            &#125;else&#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        arr[i] = temp;//将temp值放到最终的位置</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 交换元素</span><br><span class="line">     * @param arr</span><br><span class="line">     * @param a</span><br><span class="line">     * @param b</span><br><span class="line">     */</span><br><span class="line">    public static void swap(int []arr,int a ,int b)&#123;</span><br><span class="line">        int temp=arr[a];</span><br><span class="line">        arr[a] = arr[b];</span><br><span class="line">        arr[b] = temp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
图解: <a target="_blank" rel="noopener" href="https://www.cnblogs.com/chengxiao/p/6129630.html">https://www.cnblogs.com/chengxiao/p/6129630.html</a></li>
</ul>
<h2 id="归并排序-merge-sort"><a href="#归并排序-merge-sort" class="headerlink" title="归并排序(merge sort)"></a>归并排序(merge sort)</h2><p>分治，先分成最小，再合并有序数组。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">package sortdemo;</span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Created by chengxiao on 2016/12/8.</span><br><span class="line"> */</span><br><span class="line">public class MergeSort &#123;</span><br><span class="line">    public static void main(String []args)&#123;</span><br><span class="line">        int []arr = &#123;9,8,7,6,5,4,3,2,1&#125;;</span><br><span class="line">        sort(arr);</span><br><span class="line">        System.out.println(Arrays.toString(arr));</span><br><span class="line">    &#125;</span><br><span class="line">    public static void sort(int []arr)&#123;</span><br><span class="line">        int []temp = new int[arr.length];//在排序前，先建好一个长度等于原数组长度的临时数组，避免递归中频繁开辟空间</span><br><span class="line">        sort(arr,0,arr.length-1,temp);</span><br><span class="line">    &#125;</span><br><span class="line">    private static void sort(int[] arr,int left,int right,int []temp)&#123;</span><br><span class="line">        if(left&lt;right)&#123;</span><br><span class="line">            int mid = (left+right)/2;</span><br><span class="line">            sort(arr,left,mid,temp);//左边归并排序，使得左子序列有序</span><br><span class="line">            sort(arr,mid+1,right,temp);//右边归并排序，使得右子序列有序</span><br><span class="line">            merge(arr,left,mid,right,temp);//将两个有序子数组合并操作</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    private static void merge(int[] arr,int left,int mid,int right,int[] temp)&#123;</span><br><span class="line">        int i = left;//左序列指针</span><br><span class="line">        int j = mid+1;//右序列指针</span><br><span class="line">        int t = 0;//临时数组指针</span><br><span class="line">        while (i&lt;=mid &amp;&amp; j&lt;=right)&#123;</span><br><span class="line">            if(arr[i]&lt;=arr[j])&#123;</span><br><span class="line">                temp[t++] = arr[i++];</span><br><span class="line">            &#125;else &#123;</span><br><span class="line">                temp[t++] = arr[j++];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        while(i&lt;=mid)&#123;//将左边剩余元素填充进temp中</span><br><span class="line">            temp[t++] = arr[i++];</span><br><span class="line">        &#125;</span><br><span class="line">        while(j&lt;=right)&#123;//将右序列剩余元素填充进temp中</span><br><span class="line">            temp[t++] = arr[j++];</span><br><span class="line">        &#125;</span><br><span class="line">        t = 0;</span><br><span class="line">        //将temp中的元素全部拷贝到原数组中</span><br><span class="line">        while(left &lt;= right)&#123;</span><br><span class="line">            arr[left++] = temp[t++];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>图解: <a target="_blank" rel="noopener" href="https://www.cnblogs.com/chengxiao/p/6194356.html">https://www.cnblogs.com/chengxiao/p/6194356.html</a></p>
<h2 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h2><p>数据按照某种方式丢到不同的桶里面，保证左边桶的所有数字一定小于右边的所有桶的数字，比如,arr[] &#x3D; {1,2,3,1,12,32,23,45}<br>我们可以分成5个桶，通过arr[i]&#x2F;50的方式来放入，这样就变成: </p>
<ul>
<li>bucket_1 &#x3D; {1，2，3，1}</li>
<li>bucket_2 &#x3D; {12}</li>
<li>bucket_3 &#x3D; {23}</li>
<li>bucket_4 &#x3D; {32}</li>
<li>bucket_5 &#x3D; {45}<br>桶排序需要保证数据尽量均匀分布，不然会浪费桶的数量，造成额外的空间。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line">//微信公众号：bigsai</span><br><span class="line">public class test3 &#123;</span><br><span class="line">	public static void main(String[] args) &#123;</span><br><span class="line">		int a[]= &#123;1,8,7,44,42,46,38,34,33,17,15,16,27,28,24&#125;;</span><br><span class="line">		List[] buckets=new ArrayList[5];</span><br><span class="line">		for(int i=0;i&lt;buckets.length;i++)//初始化</span><br><span class="line">		&#123;</span><br><span class="line">			buckets[i]=new ArrayList&lt;Integer&gt;();</span><br><span class="line">		&#125;</span><br><span class="line">		for(int i=0;i&lt;a.length;i++)//将待排序序列放入对应桶中</span><br><span class="line">		&#123;</span><br><span class="line">			int index=a[i]/10;//对应的桶号</span><br><span class="line">			buckets[index].add(a[i]);</span><br><span class="line">		&#125;</span><br><span class="line">		for(int i=0;i&lt;buckets.length;i++)//每个桶内进行排序(使用系统自带快排)</span><br><span class="line">		&#123;</span><br><span class="line">			buckets[i].sort(null);</span><br><span class="line">			for(int j=0;j&lt;buckets[i].size();j++)//顺便打印输出</span><br><span class="line">			&#123;</span><br><span class="line">				System.out.print(buckets[i].get(j)+&quot; &quot;);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;	</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Java和python中默认的排序-timsort"><a href="#Java和python中默认的排序-timsort" class="headerlink" title="Java和python中默认的排序 - timsort"></a>Java和python中默认的排序 - timsort</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">直接看知乎吧</span><br><span class="line">https://www.zhihu.com/question/23928138</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">ray</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
